{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWcjr1oO1EenIZ+km4rC2x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBSsrrkn52Gb"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import transformers\n",
        "print(\"TF version: \", tf.__version__)\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"Transformers version: \", transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "Collecting transformers\n",
        "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
        "     |████████████████████████████████| 3.4 MB 28.4 MB/s \n",
        "Collecting pyyaml>=5.1\n",
        "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
        "     |████████████████████████████████| 596 kB 56.6 MB/s \n",
        "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
        "Collecting sacremoses\n",
        "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
        "     |████████████████████████████████| 895 kB 45.6 MB/s \n",
        "Collecting huggingface-hub<1.0,>=0.1.0\n",
        "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
        "     |████████████████████████████████| 67 kB 5.4 MB/s \n",
        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
        "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
        "Collecting tokenizers<0.11,>=0.10.1\n",
        "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
        "     |████████████████████████████████| 3.3 MB 49.3 MB/s \n",
        "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
        "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
        "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
        "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
        "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
        "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
        "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
        "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
        "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
        "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
        "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
        "  Attempting uninstall: pyyaml\n",
        "    Found existing installation: PyYAML 3.13\n",
        "    Uninstalling PyYAML-3.13:\n",
        "      Successfully uninstalled PyYAML-3.13\n",
        "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n",
        "TF version:  2.7.0\n",
        "Hub version:  0.12.0\n",
        "Transformers version:  4.15.0"
      ],
      "metadata": {
        "id": "cCCMM57i_mMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers models\n",
        "# You can add to these how ever you like\n",
        "\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from transformers import XLNetTokenizer, TFXLNetModel\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "# For text preprocessing\n",
        "import string\n",
        "import regex as re"
      ],
      "metadata": {
        "id": "qYh3cyaC-arQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing and Cleaning Data. As we are using Deep Learning we don't need to clean it a lot as Deep learning can find out about words without having seen them before.\n"
      ],
      "metadata": {
        "id": "g0Vmv0cZ-2KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep = '\\t')\n",
        "train = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep = '\\t')\n",
        "test_ids = test.PhraseId"
      ],
      "metadata": {
        "id": "04FUB6pe-0qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(3)"
      ],
      "metadata": {
        "id": "3_Wm7-LqDjpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minimal cleaning because we're using a deep learning model that can\n",
        "# learn from different variations and shapes of words\n",
        "def clean(t, punc = True, lower = True):\n",
        "\n",
        "    if lower is True:\n",
        "        t = t.lower()\n",
        "    # punctuation removal\n",
        "    if punc is True:\n",
        "        t = t.translate(str.maketrans('', '', string.punctuation))\n",
        "        \n",
        "    # removing extra space and letters\n",
        "    t = re.sub(\"\\s+\", ' ', t)\n",
        "    t = re.sub(\"\\b\\w\\b\", '', t)\n",
        "    return t\n",
        "# delete the unwanted columns\n",
        "def delete(df_list, columns):\n",
        "    for df in df_list:\n",
        "        df.drop(columns = columns, inplace = True)\n",
        "\n",
        "train['cleaned_text'] = train.Phrase.apply(lambda x: clean(x, punc = True, lower = True))\n",
        "test['cleaned_text'] = test.Phrase.apply(lambda x: clean(x, punc = True, lower = True))\n",
        "\n",
        "delete([train, test], ['Phrase', 'PhraseId', 'SentenceId'])"
      ],
      "metadata": {
        "id": "dt3lQ53TD0b_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}