{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLEetNtsGfbflz9LRp+RJC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Transformers models, no matter which one you choose, need a specific type of input to be able to process the data. The overall shape of the input is usually very similar, so it's not so hard to comprehend either."
      ],
      "metadata": {
        "id": "QWSkvl3IL6HP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHl-lawjKCq7"
      },
      "outputs": [],
      "source": [
        "models = {'roberta-large':(RobertaTokenizer,'roberta-large',TFRobertaModel),\n",
        "          #'roberta-base':(RobertaTokenizer,'roberta-base',TFRobertaModel),\n",
        "          #'bert-large':(BertTokenizer, 'bert-large-uncased', TFBertModel),\n",
        "          #'bert-base':(BertTokenizer, 'bert-base-uncased', TFBertModel),\n",
        "          #'xlnet':(XLNetTokenizer, 'xlnet-large-cased', TFXLNetModel)\n",
        "         }\n",
        "\n",
        "tokenizer, model_type, model_name = models['roberta-large']\n",
        "\n",
        "def make_inputs(tokenizer, model_type, serie, max_len= 70):\n",
        "\n",
        "    tokenizer = tokenizer.from_pretrained(model_type, lowercase=True )\n",
        "    tokenized_data = [tokenizer.encode_plus(text, max_length=max_len, \n",
        "                                            padding='max_length', \n",
        "                                            add_special_tokens=True,\n",
        "                                            truncation = True) for text in serie]\n",
        "\n",
        "    \n",
        "    input_ids = np.array([text['input_ids'] for text in tokenized_data])\n",
        "    attention_mask = np.array([text['attention_mask'] for text in tokenized_data])\n",
        "    \n",
        "    return input_ids, attention_mask\n",
        "\n",
        "input_ids_train, attention_mask_train = make_inputs(tokenizer, model_type, train.cleaned_text)\n"
      ]
    }
  ]
}